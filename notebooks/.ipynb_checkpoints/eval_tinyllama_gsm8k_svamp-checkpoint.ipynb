{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2245ef6-2177-41fd-8b3e-a6bcdd70c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b875927c-4e87-4c96-9562-128defab77d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups How big is each group of bananas?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbfbb0e-2632-4c60-ba3d-69d2f0ecc20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615606272\n",
      "Total parameters: 620,111,872\n",
      "Trainable parameters: 4,505,600\n",
      "Trainable %: 0.7266%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. Load quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# 2. Load base model in 4-bit\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "print(sum(p.numel() for p in base.parameters()))\n",
    "\n",
    "# 3. Prepare base for k-bit LoRA training\n",
    "base = prepare_model_for_kbit_training(base)\n",
    "\n",
    "# 4. Load LoRA adapter (this must point to your adapter subdir!)\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base,\n",
    "    \"../outputs/tinyllama-gsm8k-lora/adapter\",\n",
    "    is_trainable=True  # üëà **this is critical** to activate LoRA\n",
    ")\n",
    "\n",
    "# 5. Count parameters before merge\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable %: {100 * trainable_params / total_params:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a7ec16-a7eb-43e3-8ceb-132114badda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100048384\n",
      "### Question:\n",
      "There are 87 oranges and 290 bananas in Philip's collection. If the bananas are organized into 2 groups and oranges are organized into 93 groups How big is each group of bananas?\n",
      "\n",
      "### Answer:\n",
      "The bananas are organized into 93 groups, and the oranges are organized into 290 groups. Therefore, the bananas are organized into 93 groups, and the oranges are organized into 290 groups.\n"
     ]
    }
   ],
   "source": [
    "# BASE MODEL\n",
    "\n",
    "# Load base model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pad token required for batching\n",
    "\n",
    "# Inference prompt\n",
    "\n",
    "# question above\n",
    "prompt = f\"### Question:\\n{question}\\n\\n### Answer:\\n\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,      # greedy decoding\n",
    ")\n",
    "\n",
    "# Print result\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc4186b9-d0c3-4daa-87b3-e9029cf68c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eval\n",
    "import argparse, re, torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "import math\n",
    "\n",
    "NUM_RE = re.compile(r\"[-+]?\\d*\\.?\\d+\")\n",
    "\n",
    "def last_number(text: str):\n",
    "    nums = NUM_RE.findall(text)\n",
    "    return nums[-1].lstrip(\"0\") if nums else None        # strip leading zeros for fair match\n",
    "\n",
    "def build_svamp_prompt(ex):\n",
    "    if \"question_concat\" in ex:          # mirror with pre-joined field\n",
    "        qtext = ex[\"question_concat\"]\n",
    "    else:                                # ChilleD mirror ‚Üí join Body + Question\n",
    "        qtext = f\"{ex['Body'].strip()} {ex['Question'].strip()}\"\n",
    "    return f\"### Question:\\n{qtext}\\n\\n### Answer:\\n\"\n",
    "\n",
    "logs = []\n",
    "outs = []\n",
    "@torch.inference_mode()\n",
    "def accuracy(model, tok, dataset, prompt_fn, gold_fn, n=None):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    iterator = dataset if not n else dataset.select(range(n))\n",
    "    for ex in iterator:\n",
    "        prompt = prompt_fn(ex)\n",
    "        inputs = tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=False,\n",
    "        )[0]\n",
    "        \n",
    "        outs.append(tok.decode(out_ids, skip_special_tokens=True)) # take this out\n",
    "        \n",
    "        pred  = last_number(tok.decode(out_ids, skip_special_tokens=True))\n",
    "        gold  = gold_fn(ex)\n",
    "\n",
    "        logs.append((pred, gold))\n",
    "\n",
    "        if pred is not None and gold is not None and math.isclose(float(pred), float(gold)):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total if total else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb3104-0b6c-4a9b-a907-5e9e83c89f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "gsm_acc  = accuracy(model, tok, gsm8k,\n",
    "                    prompt_fn=lambda ex: f\"### Question:\\n{ex['question'].strip()}\\n\\n### Answer:\\n\",\n",
    "                    gold_fn   =lambda ex: last_number(ex[\"answer\"]),\n",
    "                    n=args.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988f32a9-6bd5-40f3-9ebe-f8ef31012c39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m svamp_acc \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_tok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mprompt_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_svamp_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgold_fn\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAnswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mosaic/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 46\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(model, tok, dataset, prompt_fn, gold_fn, n)\u001b[0m\n\u001b[1;32m     42\u001b[0m gold  \u001b[38;5;241m=\u001b[39m gold_fn(ex)\n\u001b[1;32m     44\u001b[0m logs\u001b[38;5;241m.\u001b[39mappend((pred, gold))\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m math\u001b[38;5;241m.\u001b[39misclose(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(gold)):\n\u001b[1;32m     47\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "svamp_acc = accuracy(base, base_tok, svamp,\n",
    "                     prompt_fn=build_svamp_prompt,\n",
    "                     gold_fn  =lambda ex: str(ex[\"Answer\"]).strip()\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adf7ab70-c1ec-4420-a02a-a1589f1fff12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('62', '27'),\n",
       " ('15', '4'),\n",
       " ('36', '16'),\n",
       " ('312', '64'),\n",
       " ('26', '31'),\n",
       " ('24', '720'),\n",
       " ('1', '21'),\n",
       " ('38', '64'),\n",
       " ('419', '450'),\n",
       " ('', '143550')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "041cd5a9-2772-4f29-850b-8904aa5b38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"gsm8k\", \"main\", split=\"test\")\n",
    "svamp = load_dataset(\"ChilleD/SVAMP\", split=\"test\")     # ‚úÖ works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba2be9e-ac2c-4d3e-aeb4-630f6d1e7adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading TinyLlama-1.1B base ‚Ä¶\n",
      "üîπ Loading LoRA adapter ‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# 2. base model (fp16)\n",
    "print(\"üîπ Loading TinyLlama-1.1B base ‚Ä¶\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_tok = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "base_tok.pad_token = base_tok.eos_token\n",
    "\n",
    "# 3. LoRA‚Äêfine-tuned (4-bit)\n",
    "print(\"üîπ Loading LoRA adapter ‚Ä¶\")\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "lora_base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "lora_base = prepare_model_for_kbit_training(lora_base)\n",
    "\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "    lora_base,\n",
    "    \"../outputs/tinyllama-gsm8k-lora/adapter\",   # ‚Üê your path\n",
    "    is_trainable=False,  # inference only\n",
    ")\n",
    "lora_tok = AutoTokenizer.from_pretrained(\"../outputs/tinyllama-gsm8k-lora\")\n",
    "lora_tok.pad_token = lora_tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a6c21-357c-4728-9e57-3ed926ff7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. evaluate\n",
    "print(\"\\nüöÄ Evaluating BASE model ‚Ä¶\")\n",
    "acc_base_gsm  = accuracy(base,  base_tok, gsm8k, args.n)\n",
    "acc_base_svam = accuracy(base,  base_tok, svamp, args.n)\n",
    "\n",
    "print(\"\\nüöÄ Evaluating LoRA-FINETUNED model ‚Ä¶\")\n",
    "acc_lora_gsm  = accuracy(lora_model, lora_tok, gsm8k, args.n)\n",
    "acc_lora_svam = accuracy(lora_model, lora_tok, svamp, args.n)\n",
    "\n",
    "# 5. report\n",
    "print(\"\\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê ACCURACY ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(f\"{'Model':<15} | {'GSM8K':>7} | {'SVAMP':>7}\")\n",
    "print(\"-\" * 33)\n",
    "print(f\"{'BASE':<15} | {acc_base_gsm*100:6.2f}% | {acc_base_svam*100:6.2f}%\")\n",
    "print(f\"{'LoRA-FT':<15} | {acc_lora_gsm*100:6.2f}% | {acc_lora_svam*100:6.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
